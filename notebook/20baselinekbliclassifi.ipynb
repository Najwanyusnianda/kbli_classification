{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline KBLI Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_description</th>\n",
       "      <th>kbli_code</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aktivitas: membantu menjemur cengke. produk: c...</td>\n",
       "      <td>01282</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aktivitas: tenaga honorer guru bahasa indonesi...</td>\n",
       "      <td>85230</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aktivitas: membersihkan rumput di kebun kopi. ...</td>\n",
       "      <td>01270</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aktivitas: jual kueh putu mayang keliling. pro...</td>\n",
       "      <td>47991</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aktivitas: dosen unwina (dosen tidak tetap, ma...</td>\n",
       "      <td>85321</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    text_description kbli_code text_length\n",
       "0  aktivitas: membantu menjemur cengke. produk: c...     01282          11\n",
       "1  aktivitas: tenaga honorer guru bahasa indonesi...     85230          21\n",
       "2  aktivitas: membersihkan rumput di kebun kopi. ...     01270          12\n",
       "3  aktivitas: jual kueh putu mayang keliling. pro...     47991          13\n",
       "4  aktivitas: dosen unwina (dosen tidak tetap, ma...     85321          13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_path=\"../dataset/clean/subset_kbli_classify.csv\"\n",
    "\n",
    "df=pd.read_csv(df_path,quotechar='\"',encoding=\"utf-8\",dtype=str)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103030"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kbli=df.copy()\n",
    "# Identifikasi kelas dengan minimal 2 sampel\n",
    "valid_classes = df_kbli[\"kbli_code\"].value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= 10].index\n",
    "\n",
    "# Filter dataset hanya untuk kelas valid tersebut\n",
    "df_kbli_filtered = df_kbli[df_kbli[\"kbli_code\"].isin(valid_classes)]\n",
    "\n",
    "# Ambil subset 10% stratified\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_subset, _ = train_test_split(\n",
    "    df_kbli_filtered,\n",
    "    test_size=0.5,\n",
    "    stratify=df_kbli_filtered['kbli_code'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "len(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_subset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103030"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'01111': 0, '01112': 1, '01113': 2, '01114': 3, '01115': 4, '01116': 5, '01117': 6, '01118': 7, '01119': 8, '01121': 9, '01122': 10, '01131': 11, '01132': 12, '01133': 13, '01134': 14, '01135': 15, '01136': 16, '01139': 17, '01140': 18, '01150': 19, '01160': 20, '01191': 21, '01193': 22, '01199': 23, '01220': 24, '01230': 25, '01240': 26, '01252': 27, '01253': 28, '01259': 29, '01261': 30, '01262': 31, '01269': 32, '01270': 33, '01281': 34, '01282': 35, '01283': 36, '01284': 37, '01285': 38, '01286': 39, '01289': 40, '01291': 41, '01299': 42, '01301': 43, '01302': 44, '01411': 45, '01412': 46, '01413': 47, '01420': 48, '01441': 49, '01442': 50, '01443': 51, '01450': 52, '01461': 53, '01462': 54, '01463': 55, '01464': 56, '01465': 57, '01466': 58, '01468': 59, '01469': 60, '01497': 61, '01499': 62, '01611': 63, '01612': 64, '01613': 65, '01619': 66, '01629': 67, '01630': 68, '01712': 69, '01714': 70, '01719': 71, '01724': 72, '01727': 73, '02111': 74, '02113': 75, '02119': 76, '02121': 77, '02130': 78, '02140': 79, '02201': 80, '02202': 81, '02209': 82, '02301': 83, '02302': 84, '02303': 85, '02306': 86, '02307': 87, '02308': 88, '02309': 89, '02402': 90, '02409': 91, '03111': 92, '03112': 93, '03113': 94, '03114': 95, '03115': 96, '03116': 97, '03119': 98, '03121': 99, '03122': 100, '03123': 101, '03125': 102, '03129': 103, '03131': 104, '03132': 105, '03133': 106, '03211': 107, '03212': 108, '03215': 109, '03216': 110, '03217': 111, '03219': 112, '03221': 113, '03222': 114, '03223': 115, '03224': 116, '03225': 117, '03226': 118, '03229': 119, '03231': 120, '03251': 121, '03252': 122, '03254': 123, '03255': 124, '03259': 125, '05100': 126, '06100': 127, '06201': 128, '07102': 129, '07291': 130, '07292': 131, '07293': 132, '07294': 133, '07295': 134, '07299': 135, '07301': 136, '07309': 137, '08101': 138, '08102': 139, '08103': 140, '08104': 141, '08105': 142, '08109': 143, '08930': 144, '08999': 145, '09100': 146, '09900': 147, '10110': 148, '10120': 149, '10130': 150, '10211': 151, '10212': 152, '10213': 153, '10214': 154, '10216': 155, '10219': 156, '10221': 157, '10222': 158, '10293': 159, '10298': 160, '10299': 161, '10312': 162, '10313': 163, '10320': 164, '10391': 165, '10392': 166, '10393': 167, '10399': 168, '10421': 169, '10422': 170, '10423': 171, '10431': 172, '10432': 173, '10437': 174, '10520': 175, '10531': 176, '10532': 177, '10590': 178, '10613': 179, '10614': 180, '10616': 181, '10621': 182, '10622': 183, '10631': 184, '10632': 185, '10633': 186, '10710': 187, '10721': 188, '10722': 189, '10729': 190, '10732': 191, '10734': 192, '10740': 193, '10750': 194, '10761': 195, '10763': 196, '10771': 197, '10772': 198, '10773': 199, '10774': 200, '10779': 201, '10792': 202, '10793': 203, '10794': 204, '10796': 205, '10799': 206, '10801': 207, '10802': 208, '11010': 209, '11020': 210, '11040': 211, '11051': 212, '11052': 213, '11090': 214, '12011': 215, '12012': 216, '12013': 217, '12019': 218, '12091': 219, '12099': 220, '13111': 221, '13112': 222, '13113': 223, '13121': 224, '13122': 225, '13132': 226, '13133': 227, '13134': 228, '13911': 229, '13912': 230, '13921': 231, '13923': 232, '13924': 233, '13926': 234, '13929': 235, '13930': 236, '13941': 237, '13942': 238, '13992': 239, '13999': 240, '14111': 241, '14112': 242, '14120': 243, '14131': 244, '14301': 245, '14302': 246, '15121': 247, '15129': 248, '15201': 249, '15202': 250, '15203': 251, '15209': 252, '16101': 253, '16102': 254, '16103': 255, '16104': 256, '16105': 257, '16211': 258, '16212': 259, '16213': 260, '16214': 261, '16221': 262, '16222': 263, '16230': 264, '16291': 265, '16292': 266, '16293': 267, '16294': 268, '16295': 269, '16299': 270, '17011': 271, '17012': 272, '17019': 273, '17021': 274, '17022': 275, '17091': 276, '17099': 277, '18111': 278, '18112': 279, '18113': 280, '18120': 281, '19211': 282, '20115': 283, '20118': 284, '20122': 285, '20129': 286, '20221': 287, '20231': 288, '20232': 289, '20294': 290, '21011': 291, '21012': 292, '21015': 293, '21022': 294, '22111': 295, '22112': 296, '22122': 297, '22123': 298, '22199': 299, '22220': 300, '22230': 301, '22291': 302, '22292': 303, '22293': 304, '22299': 305, '23121': 306, '23129': 307, '23911': 308, '23921': 309, '23922': 310, '23929': 311, '23932': 312, '23939': 313, '23941': 314, '23942': 315, '23951': 316, '23952': 317, '23953': 318, '23957': 319, '23959': 320, '23963': 321, '23969': 322, '23990': 323, '24101': 324, '24102': 325, '24103': 326, '24201': 327, '24202': 328, '24310': 329, '25111': 330, '25112': 331, '25119': 332, '25910': 333, '25920': 334, '25931': 335, '25932': 336, '25933': 337, '25952': 338, '25992': 339, '25993': 340, '25999': 341, '26120': 342, '26490': 343, '27320': 344, '27510': 345, '27900': 346, '28221': 347, '28224': 348, '28240': 349, '28250': 350, '29101': 351, '29200': 352, '29300': 353, '30111': 354, '30112': 355, '30911': 356, '30912': 357, '31001': 358, '31002': 359, '31004': 360, '31009': 361, '32112': 362, '32120': 363, '32201': 364, '32202': 365, '32300': 366, '32402': 367, '32901': 368, '32903': 369, '32904': 370, '32905': 371, '32909': 372, '33111': 373, '33119': 374, '33121': 375, '33122': 376, '33141': 377, '33149': 378, '33151': 379, '33159': 380, '33190': 381, '33200': 382, '35111': 383, '35112': 384, '35113': 385, '35114': 386, '35115': 387, '35117': 388, '35118': 389, '35121': 390, '35122': 391, '35129': 392, '35201': 393, '35202': 394, '35302': 395, '36001': 396, '36002': 397, '36003': 398, '38110': 399, '38211': 400, '38302': 401, '39000': 402, '41011': 403, '41012': 404, '41013': 405, '41014': 406, '41015': 407, '41016': 408, '41017': 409, '41018': 410, '41019': 411, '41020': 412, '42101': 413, '42102': 414, '42201': 415, '42202': 416, '42204': 417, '42207': 418, '42209': 419, '42911': 420, '42915': 421, '42916': 422, '42919': 423, '42929': 424, '42930': 425, '43120': 426, '43211': 427, '43212': 428, '43221': 429, '43224': 430, '43299': 431, '43302': 432, '43303': 433, '43304': 434, '43305': 435, '43309': 436, '43901': 437, '43903': 438, '43904': 439, '43905': 440, '43909': 441, '45101': 442, '45102': 443, '45103': 444, '45104': 445, '45201': 446, '45202': 447, '45301': 448, '45302': 449, '45401': 450, '45402': 451, '45403': 452, '45404': 453, '45405': 454, '45406': 455, '45407': 456, '46100': 457, '46201': 458, '46202': 459, '46204': 460, '46205': 461, '46206': 462, '46207': 463, '46209': 464, '46311': 465, '46312': 466, '46313': 467, '46314': 468, '46315': 469, '46319': 470, '46321': 471, '46322': 472, '46325': 473, '46326': 474, '46331': 475, '46332': 476, '46334': 477, '46335': 478, '46339': 479, '46411': 480, '46412': 481, '46419': 482, '46421': 483, '46422': 484, '46441': 485, '46443': 486, '46491': 487, '46499': 488, '46511': 489, '46521': 490, '46523': 491, '46591': 492, '46593': 493, '46599': 494, '46610': 495, '46620': 496, '46631': 497, '46633': 498, '46634': 499, '46636': 500, '46637': 501, '46638': 502, '46639': 503, '46652': 504, '46691': 505, '46693': 506, '46696': 507, '46699': 508, '46900': 509, '47111': 510, '47112': 511, '47191': 512, '47192': 513, '47211': 514, '47212': 515, '47213': 516, '47214': 517, '47215': 518, '47216': 519, '47219': 520, '47221': 521, '47222': 522, '47230': 523, '47241': 524, '47242': 525, '47243': 526, '47244': 527, '47245': 528, '47249': 529, '47301': 530, '47302': 531, '47303': 532, '47411': 533, '47414': 534, '47420': 535, '47511': 536, '47512': 537, '47513': 538, '47521': 539, '47523': 540, '47524': 541, '47525': 542, '47526': 543, '47527': 544, '47528': 545, '47529': 546, '47530': 547, '47591': 548, '47592': 549, '47593': 550, '47594': 551, '47595': 552, '47596': 553, '47599': 554, '47611': 555, '47612': 556, '47630': 557, '47640': 558, '47711': 559, '47712': 560, '47713': 561, '47714': 562, '47721': 563, '47722': 564, '47723': 565, '47724': 566, '47725': 567, '47729': 568, '47732': 569, '47733': 570, '47734': 571, '47735': 572, '47736': 573, '47737': 574, '47739': 575, '47741': 576, '47742': 577, '47743': 578, '47744': 579, '47749': 580, '47751': 581, '47752': 582, '47753': 583, '47754': 584, '47761': 585, '47762': 586, '47763': 587, '47771': 588, '47772': 589, '47779': 590, '47781': 591, '47789': 592, '47793': 593, '47794': 594, '47796': 595, '47797': 596, '47811': 597, '47812': 598, '47813': 599, '47814': 600, '47815': 601, '47816': 602, '47819': 603, '47821': 604, '47822': 605, '47823': 606, '47824': 607, '47825': 608, '47826': 609, '47827': 610, '47828': 611, '47829': 612, '47831': 613, '47832': 614, '47833': 615, '47834': 616, '47844': 617, '47852': 618, '47854': 619, '47859': 620, '47861': 621, '47863': 622, '47869': 623, '47877': 624, '47879': 625, '47881': 626, '47882': 627, '47891': 628, '47892': 629, '47895': 630, '47897': 631, '47899': 632, '47911': 633, '47912': 634, '47913': 635, '47914': 636, '47919': 637, '47920': 638, '47991': 639, '47992': 640, '47993': 641, '47994': 642, '47995': 643, '47996': 644, '47997': 645, '47998': 646, '47999': 647, '49110': 648, '49120': 649, '49211': 650, '49213': 651, '49214': 652, '49216': 653, '49219': 654, '49221': 655, '49229': 656, '49411': 657, '49412': 658, '49413': 659, '49414': 660, '49415': 661, '49419': 662, '49421': 663, '49422': 664, '49423': 665, '49424': 666, '49425': 667, '49426': 668, '49429': 669, '49431': 670, '49432': 671, '49433': 672, '50111': 673, '50112': 674, '50113': 675, '50114': 676, '50122': 677, '50131': 678, '50132': 679, '50133': 680, '50135': 681, '50141': 682, '50211': 683, '50212': 684, '50216': 685, '50218': 686, '50221': 687, '50222': 688, '51101': 689, '52101': 690, '52109': 691, '52212': 692, '52213': 693, '52214': 694, '52215': 695, '52219': 696, '52221': 697, '52223': 698, '52224': 699, '52225': 700, '52229': 701, '52231': 702, '52240': 703, '52291': 704, '52292': 705, '52293': 706, '52299': 707, '53100': 708, '53201': 709, '53202': 710, '55110': 711, '55120': 712, '55130': 713, '55191': 714, '55193': 715, '55194': 716, '55199': 717, '55900': 718, '56101': 719, '56102': 720, '56103': 721, '56104': 722, '56109': 723, '56210': 724, '56290': 725, '56303': 726, '56304': 727, '56305': 728, '56306': 729, '58130': 730, '59112': 731, '60101': 732, '60102': 733, '60202': 734, '61100': 735, '61200': 736, '61300': 737, '61921': 738, '61924': 739, '61929': 740, '61994': 741, '61999': 742, '62012': 743, '62019': 744, '62090': 745, '63111': 746, '63122': 747, '63912': 748, '63990': 749, '64110': 750, '64121': 751, '64122': 752, '64131': 753, '64141': 754, '64142': 755, '64143': 756, '64144': 757, '64151': 758, '64152': 759, '64190': 760, '64911': 761, '64921': 762, '64922': 763, '64999': 764, '65111': 765, '65121': 766, '65131': 767, '66199': 768, '66221': 769, '66411': 770, '66420': 771, '68111': 772, '68120': 773, '68200': 774, '69101': 775, '69102': 776, '69104': 777, '69201': 778, '69202': 779, '70100': 780, '70209': 781, '71101': 782, '71102': 783, '71202': 784, '73100': 785, '74112': 786, '74120': 787, '74130': 788, '74201': 789, '74909': 790, '75000': 791, '77100': 792, '77210': 793, '77291': 794, '77292': 795, '77295': 796, '77299': 797, '77311': 798, '77323': 799, '77392': 800, '77393': 801, '77399': 802, '78101': 803, '78103': 804, '78200': 805, '78300': 806, '79111': 807, '79112': 808, '79119': 809, '79121': 810, '79921': 811, '79990': 812, '80100': 813, '80200': 814, '81100': 815, '81210': 816, '81290': 817, '81300': 818, '82110': 819, '82190': 820, '82302': 821, '82911': 822, '82920': 823, '82990': 824, '84111': 825, '84112': 826, '84113': 827, '84114': 828, '84115': 829, '84119': 830, '84121': 831, '84122': 832, '84123': 833, '84124': 834, '84125': 835, '84126': 836, '84127': 837, '84129': 838, '84131': 839, '84132': 840, '84133': 841, '84134': 842, '84135': 843, '84136': 844, '84137': 845, '84138': 846, '84139': 847, '84221': 848, '84222': 849, '84223': 850, '84224': 851, '84231': 852, '84232': 853, '84233': 854, '84234': 855, '84300': 856, '85111': 857, '85112': 858, '85121': 859, '85122': 860, '85131': 861, '85132': 862, '85133': 863, '85134': 864, '85139': 865, '85142': 866, '85143': 867, '85151': 868, '85152': 869, '85161': 870, '85162': 871, '85210': 872, '85220': 873, '85230': 874, '85240': 875, '85251': 876, '85270': 877, '85311': 878, '85312': 879, '85321': 880, '85322': 881, '85331': 882, '85332': 883, '85410': 884, '85420': 885, '85430': 886, '85440': 887, '85451': 888, '85452': 889, '85459': 890, '85491': 891, '85492': 892, '85493': 893, '85494': 894, '85495': 895, '85499': 896, '85500': 897, '86101': 898, '86102': 899, '86103': 900, '86104': 901, '86105': 902, '86109': 903, '86201': 904, '86202': 905, '86203': 906, '86901': 907, '86902': 908, '86903': 909, '86904': 910, '88911': 911, '88991': 912, '88992': 913, '90011': 914, '90012': 915, '90022': 916, '90023': 917, '90025': 918, '90029': 919, '90090': 920, '91011': 921, '91034': 922, '93114': 923, '93116': 924, '93211': 925, '93224': 926, '93239': 927, '93244': 928, '93292': 929, '93293': 930, '93299': 931, '94110': 932, '94121': 933, '94200': 934, '94910': 935, '94920': 936, '94990': 937, '95110': 938, '95120': 939, '95210': 940, '95220': 941, '95230': 942, '95240': 943, '95291': 944, '95299': 945, '96111': 946, '96112': 947, '96121': 948, '96122': 949, '96129': 950, '96200': 951, '96910': 952, '96990': 953, '97000': 954, '98100': 955, '98200': 956}\n"
     ]
    }
   ],
   "source": [
    "## make label maps\n",
    "\n",
    "# Buat label_map dari seluruh data\n",
    "all_labels = df[\"kbli_code\"].tolist()\n",
    "label_map = {label: idx for idx, label in enumerate(sorted(set(all_labels)))}\n",
    "idx2label = {v: k for k, v in label_map.items()}\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Model Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "\n",
    "## make dataset custom\n",
    "class KBLIDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, label_map, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 208564)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = df[\"kbli_code\"].value_counts()\n",
    "valid_labels = label_counts[label_counts >=100].index\n",
    "\n",
    "df_filtered = df[df[\"kbli_code\"].isin(valid_labels)].copy()\n",
    "\n",
    "len(df_filtered)\n",
    "\n",
    "unique_code,n_sample=df_filtered[\"kbli_code\"].nunique(), len(df_kbli)\n",
    "\n",
    "(unique_code,n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'01111': 0, '01114': 1, '01115': 2, '01116': 3, '01121': 4, '01122': 5, '01131': 6, '01132': 7, '01133': 8, '01134': 9, '01135': 10, '01139': 11, '01140': 12, '01150': 13, '01220': 14, '01252': 15, '01261': 16, '01262': 17, '01270': 18, '01281': 19, '01282': 20, '01283': 21, '01284': 22, '01286': 23, '01289': 24, '01291': 25, '01299': 26, '01411': 27, '01441': 28, '01442': 29, '01450': 30, '01461': 31, '01462': 32, '01464': 33, '01613': 34, '02202': 35, '02301': 36, '02309': 37, '03111': 38, '03112': 39, '03121': 40, '03217': 41, '05100': 42, '07291': 43, '07301': 44, '08104': 45, '10421': 46, '10431': 47, '10631': 48, '10710': 49, '10722': 50, '10750': 51, '10792': 52, '10794': 53, '10799': 54, '11052': 55, '13121': 56, '13122': 57, '14111': 58, '14120': 59, '15202': 60, '16101': 61, '16211': 62, '16221': 63, '16291': 64, '16292': 65, '18111': 66, '23921': 67, '25111': 68, '31001': 69, '32903': 70, '32909': 71, '38110': 72, '41011': 73, '41012': 74, '41019': 75, '42101': 76, '42919': 77, '45201': 78, '45407': 79, '46202': 80, '46339': 81, '47111': 82, '47112': 83, '47192': 84, '47212': 85, '47213': 86, '47214': 87, '47215': 88, '47219': 89, '47241': 90, '47242': 91, '47243': 92, '47249': 93, '47301': 94, '47302': 95, '47414': 96, '47528': 97, '47711': 98, '47724': 99, '47772': 100, '47813': 101, '47815': 102, '47822': 103, '47829': 104, '47832': 105, '47912': 106, '47991': 107, '47992': 108, '47999': 109, '49422': 110, '49424': 111, '49429': 112, '49431': 113, '49432': 114, '53201': 115, '55110': 116, '55900': 117, '56101': 118, '56102': 119, '56103': 120, '56104': 121, '56210': 122, '56303': 123, '56304': 124, '56306': 125, '61999': 126, '64121': 127, '64141': 128, '68111': 129, '80100': 130, '81210': 131, '84112': 132, '84115': 133, '84119': 134, '84121': 135, '84122': 136, '84124': 137, '84125': 138, '84127': 139, '84129': 140, '84131': 141, '84137': 142, '84222': 143, '84231': 144, '85111': 145, '85112': 146, '85121': 147, '85122': 148, '85131': 149, '85132': 150, '85139': 151, '85210': 152, '85220': 153, '85230': 154, '85311': 155, '85321': 156, '85452': 157, '86101': 158, '86102': 159, '86103': 160, '86901': 161, '86902': 162, '94910': 163, '95210': 164, '96111': 165, '96112': 166, '96200': 167, '96990': 168, '97000': 169}\n"
     ]
    }
   ],
   "source": [
    "# Buat label_map dari seluruh data\n",
    "all_labels = df_filtered[\"kbli_code\"].tolist()\n",
    "label_map = {label: idx for idx, label in enumerate(sorted(set(all_labels)))}\n",
    "idx2label = {v: k for k, v in label_map.items()}\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_filtered[\"text_description\"],\n",
    "    df_filtered[\"kbli_code\"],\n",
    "    test_size=0.2,\n",
    "    stratify=df_filtered[\"kbli_code\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Dataset dan DataLoader\n",
    "train_dataset = KBLIDataset(train_texts.tolist(), train_labels.tolist(), tokenizer, label_map)\n",
    "val_dataset = KBLIDataset(val_texts.tolist(), val_labels.tolist(), tokenizer, label_map)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check sample tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([    2,  2310, 30472,  8881, 18336,    26,  1219,   448, 30470,   497,\n",
      "        30472, 18336, 30470,  1062, 30472,  6540,  2318,   536, 30470,     3,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Label ID: tensor(511)\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "print(\"Input IDs:\", sample['input_ids'])\n",
    "print(\"Attention Mask:\", sample['attention_mask'])\n",
    "print(\"Label ID:\", sample['labels'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning IndoBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class IndoBertForKBLI(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(IndoBertForKBLI, self).__init__()\n",
    "        self.indobert = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.indobert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs =self.indobert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        dropped_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(dropped_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah kelas unik (class_weights): 170\n",
      "Min label ID: 0\n",
      "Max label ID: 169\n",
      "Jumlah kelas unik: 170\n",
      "Mapping label2id: {'01111': 0, '01114': 1, '01115': 2, '01116': 3, '01121': 4, '01122': 5, '01131': 6, '01132': 7, '01133': 8, '01134': 9, '01135': 10, '01139': 11, '01140': 12, '01150': 13, '01220': 14, '01252': 15, '01261': 16, '01262': 17, '01270': 18, '01281': 19, '01282': 20, '01283': 21, '01284': 22, '01286': 23, '01289': 24, '01291': 25, '01299': 26, '01411': 27, '01441': 28, '01442': 29, '01450': 30, '01461': 31, '01462': 32, '01464': 33, '01613': 34, '02202': 35, '02301': 36, '02309': 37, '03111': 38, '03112': 39, '03121': 40, '03217': 41, '05100': 42, '07291': 43, '07301': 44, '08104': 45, '10421': 46, '10431': 47, '10631': 48, '10710': 49, '10722': 50, '10750': 51, '10792': 52, '10794': 53, '10799': 54, '11052': 55, '13121': 56, '13122': 57, '14111': 58, '14120': 59, '15202': 60, '16101': 61, '16211': 62, '16221': 63, '16291': 64, '16292': 65, '18111': 66, '23921': 67, '25111': 68, '31001': 69, '32903': 70, '32909': 71, '38110': 72, '41011': 73, '41012': 74, '41019': 75, '42101': 76, '42919': 77, '45201': 78, '45407': 79, '46202': 80, '46339': 81, '47111': 82, '47112': 83, '47192': 84, '47212': 85, '47213': 86, '47214': 87, '47215': 88, '47219': 89, '47241': 90, '47242': 91, '47243': 92, '47249': 93, '47301': 94, '47302': 95, '47414': 96, '47528': 97, '47711': 98, '47724': 99, '47772': 100, '47813': 101, '47815': 102, '47822': 103, '47829': 104, '47832': 105, '47912': 106, '47991': 107, '47992': 108, '47999': 109, '49422': 110, '49424': 111, '49429': 112, '49431': 113, '49432': 114, '53201': 115, '55110': 116, '55900': 117, '56101': 118, '56102': 119, '56103': 120, '56104': 121, '56210': 122, '56303': 123, '56304': 124, '56306': 125, '61999': 126, '64121': 127, '64141': 128, '68111': 129, '80100': 130, '81210': 131, '84112': 132, '84115': 133, '84119': 134, '84121': 135, '84122': 136, '84124': 137, '84125': 138, '84127': 139, '84129': 140, '84131': 141, '84137': 142, '84222': 143, '84231': 144, '85111': 145, '85112': 146, '85121': 147, '85122': 148, '85131': 149, '85132': 150, '85139': 151, '85210': 152, '85220': 153, '85230': 154, '85311': 155, '85321': 156, '85452': 157, '86101': 158, '86102': 159, '86103': 160, '86901': 161, '86902': 162, '94910': 163, '95210': 164, '96111': 165, '96112': 166, '96200': 167, '96990': 168, '97000': 169}\n",
      "Jumlah kelas (dari label_map): 170\n"
     ]
    }
   ],
   "source": [
    "print(\"Jumlah kelas unik (class_weights):\", len(class_weights))\n",
    "train_label_ids = [label_map[label] for label in train_labels]\n",
    "\n",
    "print(\"Min label ID:\", min(train_label_ids))\n",
    "print(\"Max label ID:\", max(train_label_ids))\n",
    "print(\"Jumlah kelas unik:\", len(set(train_label_ids)))\n",
    "\n",
    "print(\"Mapping label2id:\", label_map)\n",
    "print(\"Jumlah kelas (dari label_map):\", len(label_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83,\n",
       " 25,\n",
       " 16,\n",
       " 146,\n",
       " 73,\n",
       " 8,\n",
       " 104,\n",
       " 84,\n",
       " 16,\n",
       " 13,\n",
       " 59,\n",
       " 5,\n",
       " 92,\n",
       " 8,\n",
       " 25,\n",
       " 38,\n",
       " 5,\n",
       " 7,\n",
       " 14,\n",
       " 159,\n",
       " 10,\n",
       " 83,\n",
       " 20,\n",
       " 132,\n",
       " 16,\n",
       " 73,\n",
       " 121,\n",
       " 17,\n",
       " 0,\n",
       " 73,\n",
       " 126,\n",
       " 9,\n",
       " 52,\n",
       " 112,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 167,\n",
       " 0,\n",
       " 152,\n",
       " 10,\n",
       " 83,\n",
       " 25,\n",
       " 17,\n",
       " 6,\n",
       " 10,\n",
       " 58,\n",
       " 28,\n",
       " 159,\n",
       " 69,\n",
       " 163,\n",
       " 25,\n",
       " 128,\n",
       " 5,\n",
       " 166,\n",
       " 123,\n",
       " 83,\n",
       " 5,\n",
       " 17,\n",
       " 17,\n",
       " 41,\n",
       " 20,\n",
       " 17,\n",
       " 120,\n",
       " 25,\n",
       " 59,\n",
       " 169,\n",
       " 132,\n",
       " 145,\n",
       " 156,\n",
       " 104,\n",
       " 168,\n",
       " 5,\n",
       " 21,\n",
       " 15,\n",
       " 141,\n",
       " 5,\n",
       " 25,\n",
       " 168,\n",
       " 152,\n",
       " 10,\n",
       " 140,\n",
       " 83,\n",
       " 25,\n",
       " 8,\n",
       " 48,\n",
       " 120,\n",
       " 17,\n",
       " 43,\n",
       " 73,\n",
       " 145,\n",
       " 25,\n",
       " 10,\n",
       " 83,\n",
       " 39,\n",
       " 86,\n",
       " 119,\n",
       " 134,\n",
       " 30,\n",
       " 64,\n",
       " 58,\n",
       " 29,\n",
       " 53,\n",
       " 5,\n",
       " 13,\n",
       " 38,\n",
       " 60,\n",
       " 158,\n",
       " 75,\n",
       " 25,\n",
       " 8,\n",
       " 119,\n",
       " 116,\n",
       " 5,\n",
       " 134,\n",
       " 10,\n",
       " 73,\n",
       " 82,\n",
       " 58,\n",
       " 101,\n",
       " 9,\n",
       " 134,\n",
       " 147,\n",
       " 146,\n",
       " 25,\n",
       " 25,\n",
       " 132,\n",
       " 5,\n",
       " 0,\n",
       " 10,\n",
       " 17,\n",
       " 113,\n",
       " 31,\n",
       " 67,\n",
       " 49,\n",
       " 38,\n",
       " 5,\n",
       " 132,\n",
       " 73,\n",
       " 116,\n",
       " 158,\n",
       " 73,\n",
       " 46,\n",
       " 4,\n",
       " 146,\n",
       " 4,\n",
       " 168,\n",
       " 0,\n",
       " 107,\n",
       " 75,\n",
       " 18,\n",
       " 69,\n",
       " 5,\n",
       " 169,\n",
       " 132,\n",
       " 17,\n",
       " 82,\n",
       " 46,\n",
       " 120,\n",
       " 121,\n",
       " 5,\n",
       " 156,\n",
       " 140,\n",
       " 5,\n",
       " 9,\n",
       " 29,\n",
       " 0,\n",
       " 8,\n",
       " 108,\n",
       " 76,\n",
       " 134,\n",
       " 17,\n",
       " 27,\n",
       " 122,\n",
       " 0,\n",
       " 83,\n",
       " 10,\n",
       " 25,\n",
       " 21,\n",
       " 121,\n",
       " 5,\n",
       " 78,\n",
       " 93,\n",
       " 73,\n",
       " 29,\n",
       " 8,\n",
       " 113,\n",
       " 5,\n",
       " 83,\n",
       " 38,\n",
       " 5,\n",
       " 86,\n",
       " 0,\n",
       " 47,\n",
       " 83,\n",
       " 9,\n",
       " 83,\n",
       " 68,\n",
       " 59,\n",
       " 90,\n",
       " 62,\n",
       " 126,\n",
       " 163,\n",
       " 164,\n",
       " 24,\n",
       " 5,\n",
       " 73,\n",
       " 53,\n",
       " 119,\n",
       " 20,\n",
       " 123,\n",
       " 5,\n",
       " 5,\n",
       " 14,\n",
       " 124,\n",
       " 83,\n",
       " 7,\n",
       " 124,\n",
       " 18,\n",
       " 17,\n",
       " 132,\n",
       " 113,\n",
       " 73,\n",
       " 57,\n",
       " 91,\n",
       " 24,\n",
       " 126,\n",
       " 6,\n",
       " 20,\n",
       " 111,\n",
       " 25,\n",
       " 152,\n",
       " 1,\n",
       " 118,\n",
       " 7,\n",
       " 128,\n",
       " 120,\n",
       " 139,\n",
       " 109,\n",
       " 5,\n",
       " 29,\n",
       " 168,\n",
       " 17,\n",
       " 10,\n",
       " 73,\n",
       " 46,\n",
       " 134,\n",
       " 119,\n",
       " 34,\n",
       " 120,\n",
       " 84,\n",
       " 29,\n",
       " 59,\n",
       " 10,\n",
       " 13,\n",
       " 77,\n",
       " 86,\n",
       " 135,\n",
       " 18,\n",
       " 25,\n",
       " 17,\n",
       " 168,\n",
       " 145,\n",
       " 154,\n",
       " 61,\n",
       " 25,\n",
       " 25,\n",
       " 5,\n",
       " 5,\n",
       " 58,\n",
       " 52,\n",
       " 24,\n",
       " 83,\n",
       " 41,\n",
       " 18,\n",
       " 25,\n",
       " 156,\n",
       " 120,\n",
       " 0,\n",
       " 84,\n",
       " 25,\n",
       " 118,\n",
       " 38,\n",
       " 23,\n",
       " 136,\n",
       " 10,\n",
       " 38,\n",
       " 25,\n",
       " 41,\n",
       " 113,\n",
       " 29,\n",
       " 5,\n",
       " 38,\n",
       " 79,\n",
       " 24,\n",
       " 17,\n",
       " 18,\n",
       " 83,\n",
       " 6,\n",
       " 83,\n",
       " 8,\n",
       " 118,\n",
       " 121,\n",
       " 36,\n",
       " 83,\n",
       " 60,\n",
       " 17,\n",
       " 83,\n",
       " 10,\n",
       " 0,\n",
       " 144,\n",
       " 18,\n",
       " 120,\n",
       " 18,\n",
       " 107,\n",
       " 148,\n",
       " 83,\n",
       " 0,\n",
       " 105,\n",
       " 93,\n",
       " 107,\n",
       " 5,\n",
       " 44,\n",
       " 25,\n",
       " 84,\n",
       " 141,\n",
       " 49,\n",
       " 21,\n",
       " 40,\n",
       " 0,\n",
       " 6,\n",
       " 18,\n",
       " 83,\n",
       " 103,\n",
       " 6,\n",
       " 97,\n",
       " 13,\n",
       " 83,\n",
       " 107,\n",
       " 73,\n",
       " 73,\n",
       " 5,\n",
       " 119,\n",
       " 44,\n",
       " 0,\n",
       " 18,\n",
       " 17,\n",
       " 13,\n",
       " 17,\n",
       " 134,\n",
       " 10,\n",
       " 78,\n",
       " 107,\n",
       " 59,\n",
       " 132,\n",
       " 25,\n",
       " 36,\n",
       " 135,\n",
       " 113,\n",
       " 32,\n",
       " 42,\n",
       " 5,\n",
       " 11,\n",
       " 83,\n",
       " 97,\n",
       " 5,\n",
       " 120,\n",
       " 158,\n",
       " 17,\n",
       " 8,\n",
       " 23,\n",
       " 98,\n",
       " 44,\n",
       " 59,\n",
       " 83,\n",
       " 4,\n",
       " 101,\n",
       " 119,\n",
       " 119,\n",
       " 151,\n",
       " 5,\n",
       " 114,\n",
       " 9,\n",
       " 48,\n",
       " 61,\n",
       " 145,\n",
       " 73,\n",
       " 17,\n",
       " 16,\n",
       " 13,\n",
       " 9,\n",
       " 78,\n",
       " 40,\n",
       " 17,\n",
       " 111,\n",
       " 145,\n",
       " 5,\n",
       " 17,\n",
       " 29,\n",
       " 119,\n",
       " 6,\n",
       " 168,\n",
       " 6,\n",
       " 136,\n",
       " 119,\n",
       " 30,\n",
       " 25,\n",
       " 8,\n",
       " 61,\n",
       " 120,\n",
       " 18,\n",
       " 10,\n",
       " 123,\n",
       " 161,\n",
       " 132,\n",
       " 44,\n",
       " 10,\n",
       " 111,\n",
       " 83,\n",
       " 27,\n",
       " 5,\n",
       " 51,\n",
       " 83,\n",
       " 18,\n",
       " 83,\n",
       " 79,\n",
       " 5,\n",
       " 6,\n",
       " 120,\n",
       " 83,\n",
       " 10,\n",
       " 145,\n",
       " 141,\n",
       " 143,\n",
       " 9,\n",
       " 64,\n",
       " 17,\n",
       " 25,\n",
       " 79,\n",
       " 49,\n",
       " 27,\n",
       " 5,\n",
       " 51,\n",
       " 58,\n",
       " 120,\n",
       " 6,\n",
       " 134,\n",
       " 17,\n",
       " 5,\n",
       " 52,\n",
       " 5,\n",
       " 25,\n",
       " 144,\n",
       " 17,\n",
       " 73,\n",
       " 83,\n",
       " 17,\n",
       " 82,\n",
       " 92,\n",
       " 12,\n",
       " 119,\n",
       " 126,\n",
       " 46,\n",
       " 132,\n",
       " 110,\n",
       " 38,\n",
       " 50,\n",
       " 27,\n",
       " 69,\n",
       " 143,\n",
       " 158,\n",
       " 81,\n",
       " 119,\n",
       " 25,\n",
       " 14,\n",
       " 145,\n",
       " 23,\n",
       " 0,\n",
       " 40,\n",
       " 20,\n",
       " 157,\n",
       " 17,\n",
       " 5,\n",
       " 5,\n",
       " 101,\n",
       " 5,\n",
       " 31,\n",
       " 69,\n",
       " 9,\n",
       " 42,\n",
       " 24,\n",
       " 168,\n",
       " 73,\n",
       " 6,\n",
       " 9,\n",
       " 83,\n",
       " 83,\n",
       " 75,\n",
       " 5,\n",
       " 120,\n",
       " 132,\n",
       " 6,\n",
       " 67,\n",
       " 64,\n",
       " 27,\n",
       " 152,\n",
       " 18,\n",
       " 42,\n",
       " 81,\n",
       " 29,\n",
       " 71,\n",
       " 91,\n",
       " 147,\n",
       " 91,\n",
       " 17,\n",
       " 38,\n",
       " 5,\n",
       " 168,\n",
       " 96,\n",
       " 121,\n",
       " 43,\n",
       " 119,\n",
       " 5,\n",
       " 5,\n",
       " 120,\n",
       " 7,\n",
       " 17,\n",
       " 119,\n",
       " 31,\n",
       " 146,\n",
       " 73,\n",
       " 145,\n",
       " 5,\n",
       " 5,\n",
       " 57,\n",
       " 44,\n",
       " 0,\n",
       " 27,\n",
       " 120,\n",
       " 0,\n",
       " 9,\n",
       " 121,\n",
       " 95,\n",
       " 169,\n",
       " 5,\n",
       " 113,\n",
       " 6,\n",
       " 113,\n",
       " 119,\n",
       " 6,\n",
       " 44,\n",
       " 18,\n",
       " 5,\n",
       " 4,\n",
       " 60,\n",
       " 83,\n",
       " 64,\n",
       " 38,\n",
       " 120,\n",
       " 73,\n",
       " 73,\n",
       " 46,\n",
       " 5,\n",
       " 10,\n",
       " 83,\n",
       " 18,\n",
       " 26,\n",
       " 11,\n",
       " 16,\n",
       " 5,\n",
       " 24,\n",
       " 17,\n",
       " 0,\n",
       " 149,\n",
       " 15,\n",
       " 168,\n",
       " 17,\n",
       " 5,\n",
       " 169,\n",
       " 25,\n",
       " 73,\n",
       " 17,\n",
       " 17,\n",
       " 6,\n",
       " 145,\n",
       " 73,\n",
       " 108,\n",
       " 59,\n",
       " 83,\n",
       " 40,\n",
       " 105,\n",
       " 145,\n",
       " 32,\n",
       " 73,\n",
       " 73,\n",
       " 120,\n",
       " 144,\n",
       " 27,\n",
       " 159,\n",
       " 120,\n",
       " 0,\n",
       " 20,\n",
       " 147,\n",
       " 25,\n",
       " 25,\n",
       " 0,\n",
       " 79,\n",
       " 34,\n",
       " 98,\n",
       " 83,\n",
       " 17,\n",
       " 25,\n",
       " 25,\n",
       " 5,\n",
       " 5,\n",
       " 137,\n",
       " 18,\n",
       " 98,\n",
       " 38,\n",
       " 126,\n",
       " 102,\n",
       " 53,\n",
       " 76,\n",
       " 145,\n",
       " 150,\n",
       " 13,\n",
       " 145,\n",
       " 57,\n",
       " 17,\n",
       " 30,\n",
       " 115,\n",
       " 143,\n",
       " 38,\n",
       " 101,\n",
       " 83,\n",
       " 9,\n",
       " 147,\n",
       " 52,\n",
       " 5,\n",
       " 5,\n",
       " 159,\n",
       " 83,\n",
       " 161,\n",
       " 88,\n",
       " 132,\n",
       " 80,\n",
       " 89,\n",
       " 5,\n",
       " 5,\n",
       " 73,\n",
       " 38,\n",
       " 79,\n",
       " 18,\n",
       " 121,\n",
       " 64,\n",
       " 18,\n",
       " 83,\n",
       " 34,\n",
       " 17,\n",
       " 132,\n",
       " 5,\n",
       " 146,\n",
       " 146,\n",
       " 132,\n",
       " 145,\n",
       " 83,\n",
       " 140,\n",
       " 67,\n",
       " 23,\n",
       " 5,\n",
       " 58,\n",
       " 159,\n",
       " 121,\n",
       " 119,\n",
       " 10,\n",
       " 38,\n",
       " 169,\n",
       " 16,\n",
       " 16,\n",
       " 38,\n",
       " 87,\n",
       " 84,\n",
       " 100,\n",
       " 89,\n",
       " 119,\n",
       " 5,\n",
       " 93,\n",
       " 73,\n",
       " 145,\n",
       " 73,\n",
       " 18,\n",
       " 101,\n",
       " 17,\n",
       " 69,\n",
       " 166,\n",
       " 84,\n",
       " 26,\n",
       " 10,\n",
       " 24,\n",
       " 17,\n",
       " 119,\n",
       " 158,\n",
       " 58,\n",
       " 8,\n",
       " 111,\n",
       " 5,\n",
       " 79,\n",
       " 83,\n",
       " 162,\n",
       " 83,\n",
       " 6,\n",
       " 148,\n",
       " 6,\n",
       " 10,\n",
       " 5,\n",
       " 53,\n",
       " 18,\n",
       " 72,\n",
       " 59,\n",
       " 69,\n",
       " 154,\n",
       " 16,\n",
       " 0,\n",
       " 59,\n",
       " 0,\n",
       " 10,\n",
       " 18,\n",
       " 18,\n",
       " 5,\n",
       " 146,\n",
       " 16,\n",
       " 0,\n",
       " 7,\n",
       " 120,\n",
       " 25,\n",
       " 17,\n",
       " 120,\n",
       " 139,\n",
       " 157,\n",
       " 83,\n",
       " 9,\n",
       " 25,\n",
       " 120,\n",
       " 111,\n",
       " 63,\n",
       " 149,\n",
       " 5,\n",
       " 153,\n",
       " 134,\n",
       " 17,\n",
       " 7,\n",
       " 18,\n",
       " 18,\n",
       " 5,\n",
       " 38,\n",
       " 158,\n",
       " 10,\n",
       " 132,\n",
       " 148,\n",
       " 83,\n",
       " 29,\n",
       " 8,\n",
       " 4,\n",
       " 49,\n",
       " 111,\n",
       " 21,\n",
       " 6,\n",
       " 152,\n",
       " 83,\n",
       " 83,\n",
       " 154,\n",
       " 4,\n",
       " 38,\n",
       " 27,\n",
       " 0,\n",
       " 16,\n",
       " 139,\n",
       " 25,\n",
       " 169,\n",
       " 17,\n",
       " 136,\n",
       " 93,\n",
       " 73,\n",
       " 61,\n",
       " 19,\n",
       " 64,\n",
       " 28,\n",
       " 122,\n",
       " 38,\n",
       " 159,\n",
       " 115,\n",
       " 5,\n",
       " 119,\n",
       " 8,\n",
       " 6,\n",
       " 98,\n",
       " 4,\n",
       " 10,\n",
       " 42,\n",
       " 119,\n",
       " 16,\n",
       " 73,\n",
       " 119,\n",
       " 6,\n",
       " 73,\n",
       " 20,\n",
       " 0,\n",
       " 25,\n",
       " 10,\n",
       " 146,\n",
       " 168,\n",
       " 52,\n",
       " 83,\n",
       " 164,\n",
       " 25,\n",
       " 0,\n",
       " 132,\n",
       " 18,\n",
       " 148,\n",
       " 17,\n",
       " 18,\n",
       " 68,\n",
       " 120,\n",
       " 169,\n",
       " 21,\n",
       " 5,\n",
       " 10,\n",
       " 82,\n",
       " 145,\n",
       " 0,\n",
       " 9,\n",
       " 17,\n",
       " 154,\n",
       " 8,\n",
       " 108,\n",
       " 27,\n",
       " 1,\n",
       " 119,\n",
       " 21,\n",
       " 44,\n",
       " 123,\n",
       " 4,\n",
       " 106,\n",
       " 52,\n",
       " 21,\n",
       " 16,\n",
       " 8,\n",
       " 83,\n",
       " 73,\n",
       " 83,\n",
       " 119,\n",
       " 83,\n",
       " 25,\n",
       " 73,\n",
       " 5,\n",
       " 52,\n",
       " 116,\n",
       " 10,\n",
       " 10,\n",
       " 169,\n",
       " 5,\n",
       " 25,\n",
       " 84,\n",
       " 117,\n",
       " 5,\n",
       " 76,\n",
       " 5,\n",
       " 145,\n",
       " 0,\n",
       " 25,\n",
       " 17,\n",
       " 25,\n",
       " 13,\n",
       " 27,\n",
       " 168,\n",
       " 150,\n",
       " 95,\n",
       " 94,\n",
       " 5,\n",
       " 29,\n",
       " 5,\n",
       " 18,\n",
       " 107,\n",
       " 151,\n",
       " 29,\n",
       " 55,\n",
       " 166,\n",
       " 18,\n",
       " 17,\n",
       " 0,\n",
       " 27,\n",
       " 10,\n",
       " 72,\n",
       " 66,\n",
       " 0,\n",
       " 17,\n",
       " 146,\n",
       " 0,\n",
       " 91,\n",
       " 86,\n",
       " 89,\n",
       " 82,\n",
       " 83,\n",
       " 103,\n",
       " 27,\n",
       " 25,\n",
       " 56,\n",
       " 132,\n",
       " 0,\n",
       " 83,\n",
       " 89,\n",
       " 47,\n",
       " 135,\n",
       " 154,\n",
       " 10,\n",
       " 0,\n",
       " 71,\n",
       " 119,\n",
       " 147,\n",
       " 83,\n",
       " 0,\n",
       " 116,\n",
       " 79,\n",
       " 25,\n",
       " 77,\n",
       " 102,\n",
       " 151,\n",
       " 49,\n",
       " 25,\n",
       " 5,\n",
       " 38,\n",
       " 73,\n",
       " 17,\n",
       " 47,\n",
       " 25,\n",
       " 10,\n",
       " 0,\n",
       " 46,\n",
       " 145,\n",
       " 17,\n",
       " 27,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 143,\n",
       " 16,\n",
       " 73,\n",
       " 168,\n",
       " 46,\n",
       " 111,\n",
       " 73,\n",
       " 165,\n",
       " 38,\n",
       " 5,\n",
       " 93,\n",
       " 61,\n",
       " 20,\n",
       " 73,\n",
       " 126,\n",
       " 83,\n",
       " 14,\n",
       " 61,\n",
       " 42,\n",
       " 29,\n",
       " 10,\n",
       " 73,\n",
       " 5,\n",
       " 27,\n",
       " 18,\n",
       " 83,\n",
       " 25,\n",
       " 91,\n",
       " 88,\n",
       " 166,\n",
       " 121,\n",
       " 17,\n",
       " 17,\n",
       " 109,\n",
       " 142,\n",
       " 143,\n",
       " 36,\n",
       " 36,\n",
       " 121,\n",
       " 11,\n",
       " 93,\n",
       " 79,\n",
       " 146,\n",
       " 46,\n",
       " 156,\n",
       " 88,\n",
       " 27,\n",
       " 74,\n",
       " 69,\n",
       " 6,\n",
       " 38,\n",
       " 152,\n",
       " 5,\n",
       " 25,\n",
       " 16,\n",
       " 14,\n",
       " ...]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_ids = [label_map[label] for label in train_labels]\n",
    "train_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlabel_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "label_map.shape(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 16\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Hitung class weight dari ID, bukan string\u001b[39;00m\n\u001b[0;32m     10\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m compute_class_weight(\n\u001b[0;32m     11\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(label_map)),  \u001b[38;5;66;03m# semua ID label, misal 0956\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     y\u001b[38;5;241m=\u001b[39mtrain_label_ids\n\u001b[0;32m     14\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m class_weights\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m## init model\u001b[39;00m\n\u001b[0;32m     19\u001b[0m num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(train_labels))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ubah label string ke ID berdasarkan label_map\n",
    "train_label_ids = [label_map[label] for label in train_labels]\n",
    "# Hitung class weight dari ID, bukan string\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.arange(len(label_map)),  # semua ID label, misal 0956\n",
    "    y=train_label_ids\n",
    ")\n",
    "\n",
    "class_weights=torch.tensor(class_weights,dtype=torch.float).to(device)\n",
    "\n",
    "## init model\n",
    "num_labels=len(np.unique(train_labels))\n",
    "model=IndoBertForKBLI(num_labels=num_labels).to(device)\n",
    "\n",
    "## init optimizer\n",
    "optimizer=AdamW(model.parameters(),lr=1e-5)\n",
    "\n",
    "## init loss\n",
    "loss_fn=torch.nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/719 [00:01<04:09,  2.87it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     20\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 22\u001b[0m         epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRata-rata Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m##save model\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 5\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device).long()  # Ensure labels are of type LongTensor\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Rata-rata Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "##save model\n",
    "torch.save(model.state_dict(), \"../outputs/checkpoints/indobert_kbli_baseline.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../outputs/checkpoints/indobert_kbli_baseline.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score (macro): 0.0003387927801395947\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(all_labels, all_preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1-Score (macro):\u001b[39m\u001b[38;5;124m\"\u001b[39m, f1)\n\u001b[1;32m---> 23\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m(all_labels, all_preds)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, acc)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Skor F1\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "print(\"F1-Score (macro):\", f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0019193857965451055\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: tensor([700,  75, 218, 513, 413, 862,  53, 862], device='cuda:0')\n",
      "Label: tensor([397, 608,   4, 286, 251, 487,   2, 487], device='cuda:0')\n",
      "Jumlah kelas: 1101\n",
      "Shape output logits: torch.Size([8, 1101])\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred:\", preds[:10])\n",
    "print(\"Label:\", labels[:10])\n",
    "print(\"Jumlah kelas:\", len(set(train_labels)))\n",
    "print(\"Shape output logits:\", outputs.shape)  # Harus (batch_size, num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 3]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(all_labels, all_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
